#!/usr/bin/env python
"""
Case Study V2: Single Mask Level (5%) Loss Analysis

æ”¹è¿›æ–¹æ¡ˆï¼š
- åªä½¿ç”¨ 5% æ©ç ç‡
- ç›´æ¥ä½¿ç”¨ Lossï¼ˆä¸è½¬æ¢ä¸º PPLï¼‰
- å¤šæ¬¡éšæœºé‡‡æ ·å–å¹³å‡ï¼ˆå‡å°‘éšæœºæ€§ï¼‰
"""

import os
import sys
import argparse
import json
import yaml
from typing import List, Dict, Tuple

import numpy as np
import torch
import torch.nn.functional as F
from datasets import Dataset, load_dataset as hf_load_dataset
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 11

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from attack.misc.models import ModelManager
from attack.misc.utils import set_seed, resolve_path
from attack.attacks.utils import get_model_nll_params


def load_local_json_dataset(train_path: str, test_path: str) -> Dataset:
    """åŠ è½½æœ¬åœ° JSON Lines æ•°æ®é›†"""
    from datasets import Dataset as HFDataset

    all_data = []

    def load_jsonl_file(file_path, default_label):
        """åŠ è½½ JSON Lines æ–‡ä»¶"""
        if not os.path.exists(file_path):
            print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return []

        print(f"  ğŸ“‚ åŠ è½½:  {file_path}")
        items = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        item = json.loads(line)
                        text = item.get('text')
                        if text is None:
                            continue

                        label = item.get('label', default_label)
                        items.append({'text': str(text), 'label': int(label)})

                    except json.JSONDecodeError:
                        continue

                    if line_num % 1000 == 0:
                        print(f"     å·²åŠ è½½ {line_num} è¡Œ.. .", end='\r')

            print(f"     âœ… åŠ è½½ {len(items)} æ¡æ•°æ®" + " " * 20)

        except Exception as e:
            print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
            return []

        return items

    # åŠ è½½è®­ç»ƒé›†ï¼ˆmember, label=1ï¼‰
    print("\nåŠ è½½è®­ç»ƒé›† (Member):")
    train_items = load_jsonl_file(train_path, default_label=1)
    all_data.extend(train_items)

    # åŠ è½½æµ‹è¯•é›†ï¼ˆnon-member, label=0ï¼‰
    print("\nåŠ è½½æµ‹è¯•é›† (Non-member):")
    test_items = load_jsonl_file(test_path, default_label=0)
    all_data.extend(test_items)

    if not all_data:
        raise ValueError("æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®")

    print(f"\nâœ… æ€»è®¡åŠ è½½ {len(all_data)} æ¡æ•°æ®")
    print(f"   - Member (label=1): {sum(1 for x in all_data if x['label'] == 1)}")
    print(f"   - Non-member (label=0): {sum(1 for x in all_data if x['label'] == 0)}")

    return HFDataset.from_list(all_data)


class LossAnalyzer:
    """Loss åˆ†æå™¨ - åªä½¿ç”¨ 5% æ©ç """

    def __init__(self, model, tokenizer, device, config):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.config = config

        # è·å–æ¨¡å‹ç‰¹å®šå‚æ•°
        self.mask_id, self.shift_logits = get_model_nll_params(self.model)

        # æ©ç é…ç½®
        self.mask_frac = config.get('mask_frac', 0.05)  # å›ºå®š 5%
        self.num_runs = config.get('num_runs', 5)  # å¤šæ¬¡é‡‡æ ·
        self.max_length = config.get('max_length', 512)

        print(f"[LossAnalyzer] åˆå§‹åŒ–å®Œæˆ")
        print(f"  - æ©ç æ¯”ä¾‹: {self.mask_frac:.1%}")
        print(f"  - é‡‡æ ·æ¬¡æ•°:  {self.num_runs}")
        print(f"  - Mask ID: {self.mask_id}, Shift Logits: {self.shift_logits}")
        print(f"  - Max Length: {self.max_length}")

    @torch.no_grad()
    def analyze_single_text(self, text: str) -> Dict:
        """
        åˆ†æå•ä¸ªæ–‡æœ¬

        Returns:
            {
                'losses': [loss1, loss2, ...],  # æ¯æ¬¡é‡‡æ ·çš„ loss
                'mean_loss': float,              # å¹³å‡ loss
                'std_loss': float,               # loss æ ‡å‡†å·®
                'min_loss': float,               # æœ€å° loss
                'max_loss': float                # æœ€å¤§ loss
            }
        """
        # Tokenize
        encoded = self.tokenizer.encode_plus(
            text,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=self.max_length
        ).to(self.device)

        input_ids = encoded["input_ids"]  # (1, L)
        attention_mask = encoded["attention_mask"].bool()  # (1, L)
        L = input_ids.size(1)

        # æœ‰æ•ˆé•¿åº¦
        valid_length = int(attention_mask.sum().item())
        if valid_length == 0:
            return {'losses': [], 'mean_loss': float('inf'), 'std_loss': 0,
                    'min_loss': float('inf'), 'max_loss': float('inf')}

        # è®¡ç®—éœ€è¦æ©ç çš„ token æ•°é‡
        num_to_mask = max(1, int(round(self.mask_frac * valid_length)))

        losses = []

        # å¤šæ¬¡éšæœºé‡‡æ ·
        for run in range(self.num_runs):
            # éšæœºé€‰æ‹©æ©ç ä½ç½®
            valid_positions = torch.where(attention_mask[0])[0]

            if len(valid_positions) < num_to_mask:
                num_to_mask = len(valid_positions)

            perm = torch.randperm(len(valid_positions), device=self.device)
            chosen_indices = valid_positions[perm[: num_to_mask]]

            # åˆ›å»ºæ©ç 
            mask = torch.zeros_like(input_ids, dtype=torch.bool)
            mask[0, chosen_indices] = True

            # æ©ç è¾“å…¥
            masked_ids = input_ids.clone()
            masked_ids[mask] = self.mask_id

            # å‰å‘ä¼ æ’­
            out = self.model(
                input_ids=masked_ids,
                attention_mask=attention_mask if not self.shift_logits else None
            )
            logits = out.logits if hasattr(out, 'logits') else out[0]

            if self.shift_logits:
                logits = torch.cat([logits[:, :1, :], logits[:, :-1, :]], dim=1)

            # è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼ˆåªé’ˆå¯¹æ©ç ä½ç½®ï¼‰
            ce = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                input_ids.view(-1),
                reduction='none'
            ).view(1, L).float()

            # å¹³å‡ loss
            avg_loss = ce[mask].mean().item()
            losses.append(avg_loss)

        # ç»Ÿè®¡
        losses = np.array(losses)
        return {
            'losses': losses.tolist(),
            'mean_loss': float(np.mean(losses)),
            'std_loss': float(np.std(losses)),
            'min_loss': float(np.min(losses)),
            'max_loss': float(np.max(losses))
        }

    def analyze_dataset(self, dataset, num_samples_per_class: int = 20):
        """
        åˆ†ææ•°æ®é›†ä¸­çš„æ ·æœ¬

        Returns:
            results: Dict with member/non-member results
        """
        results = {
            'member': {'texts': [], 'losses': [], 'mean_losses': [], 'std_losses': []},
            'non_member': {'texts': [], 'losses': [], 'mean_losses': [], 'std_losses': []}
        }

        # åˆ†ç¦» member å’Œ non-member
        member_indices = [i for i, label in enumerate(dataset['label']) if label == 1]
        non_member_indices = [i for i, label in enumerate(dataset['label']) if label == 0]

        print(f"\næ•°æ®é›†ç»Ÿè®¡:")
        print(f"  - Member æ ·æœ¬: {len(member_indices)}")
        print(f"  - Non-member æ ·æœ¬: {len(non_member_indices)}")

        # éšæœºé‡‡æ ·
        np.random.seed(42)
        selected_member = np.random.choice(member_indices,
                                           min(num_samples_per_class, len(member_indices)),
                                           replace=False)
        selected_non_member = np.random.choice(non_member_indices,
                                               min(num_samples_per_class, len(non_member_indices)),
                                               replace=False)

        # åˆ†æ member æ ·æœ¬
        print(f"\nåˆ†æ {len(selected_member)} ä¸ª Member æ ·æœ¬...")
        for idx in tqdm(selected_member, desc="Member"):
            text = dataset[int(idx)]['text']
            result = self.analyze_single_text(text)

            if result['losses']:
                results['member']['texts'].append(text[: 100])
                results['member']['losses'].append(result['losses'])
                results['member']['mean_losses'].append(result['mean_loss'])
                results['member']['std_losses'].append(result['std_loss'])

        # åˆ†æ non-member æ ·æœ¬
        print(f"åˆ†æ {len(selected_non_member)} ä¸ª Non-member æ ·æœ¬...")
        for idx in tqdm(selected_non_member, desc="Non-member"):
            text = dataset[int(idx)]['text']
            result = self.analyze_single_text(text)

            if result['losses']:
                results['non_member']['texts'].append(text[:100])
                results['non_member']['losses'].append(result['losses'])
                results['non_member']['mean_losses'].append(result['mean_loss'])
                results['non_member']['std_losses'].append(result['std_loss'])

        return results


def plot_comprehensive_analysis(results: Dict, output_dir: str, config: Dict):
    """ç»˜åˆ¶ç»¼åˆåˆ†æå›¾"""
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»º 2x3 å­å›¾å¸ƒå±€
    fig = plt.figure(figsize=(18, 12))

    member_losses = np.array(results['member']['mean_losses'])
    non_member_losses = np.array(results['non_member']['mean_losses'])

    # === å­å›¾ 1: Loss åˆ†å¸ƒï¼ˆç›´æ–¹å›¾ï¼‰ ===
    ax1 = plt.subplot(2, 3, 1)
    ax1.hist(member_losses, bins=15, alpha=0.6, label='Member', color='#2E86AB', edgecolor='black')
    ax1.hist(non_member_losses, bins=15, alpha=0.6, label='Non-Member', color='#A23B72', edgecolor='black')
    ax1.axvline(np.mean(member_losses), color='#2E86AB', linestyle='--', linewidth=2,
                label=f'Member Mean: {np.mean(member_losses):.3f}')
    ax1.axvline(np.mean(non_member_losses), color='#A23B72', linestyle='--', linewidth=2,
                label=f'Non-Member Mean:  {np.mean(non_member_losses):.3f}')
    ax1.set_xlabel('Loss (5% Mask)', fontweight='bold')
    ax1.set_ylabel('Frequency')
    ax1.set_title('Loss Distribution Comparison', fontweight='bold', fontsize=13)
    ax1.legend(fontsize=9)
    ax1.grid(True, alpha=0.3, axis='y')

    # === å­å›¾ 2: Loss åˆ†å¸ƒï¼ˆç®±çº¿å›¾ï¼‰ ===
    ax2 = plt.subplot(2, 3, 2)
    box_data = [member_losses, non_member_losses]
    bp = ax2.boxplot(box_data, labels=['Member', 'Non-Member'],
                     patch_artist=True, widths=0.6)
    bp['boxes'][0].set_facecolor('#2E86AB')
    bp['boxes'][1].set_facecolor('#A23B72')
    for median in bp['medians']:
        median.set(color='red', linewidth=2)
    means = [np.mean(member_losses), np.mean(non_member_losses)]
    ax2.scatter([1, 2], means, color='yellow', s=100, zorder=3,
                edgecolors='black', linewidths=2, label='Mean')
    ax2.set_ylabel('Loss (5% Mask)', fontweight='bold')
    ax2.set_title('Loss Distribution (Box Plot)', fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3, axis='y')

    # === å­å›¾ 3: ä¸ªä½“ Loss å¯¹æ¯” ===
    ax3 = plt.subplot(2, 3, 3)
    x_member = np.arange(len(member_losses))
    x_non_member = np.arange(len(non_member_losses))
    ax3.scatter(x_member, member_losses, alpha=0.6, label='Member', color='#2E86AB', s=50)
    ax3.scatter(x_non_member, non_member_losses, alpha=0.6, label='Non-Member', color='#A23B72', s=50, marker='s')
    ax3.axhline(np.mean(member_losses), color='#2E86AB', linestyle='--', linewidth=2)
    ax3.axhline(np.mean(non_member_losses), color='#A23B72', linestyle='--', linewidth=2)
    ax3.set_xlabel('Sample Index', fontweight='bold')
    ax3.set_ylabel('Loss', fontweight='bold')
    ax3.set_title('Individual Loss Values', fontweight='bold')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # === å­å›¾ 4: é‡‡æ ·ç¨³å®šæ€§åˆ†æ ===
    ax4 = plt.subplot(2, 3, 4)
    member_stds = np.array(results['member']['std_losses'])
    non_member_stds = np.array(results['non_member']['std_losses'])
    ax4.hist(member_stds, bins=15, alpha=0.6, label='Member', color='#2E86AB', edgecolor='black')
    ax4.hist(non_member_stds, bins=15, alpha=0.6, label='Non-Member', color='#A23B72', edgecolor='black')
    ax4.set_xlabel('Loss Std Dev (across runs)', fontweight='bold')
    ax4.set_ylabel('Frequency')
    ax4.set_title('Sampling Stability', fontweight='bold')
    ax4.legend()
    ax4.grid(True, alpha=0.3, axis='y')

    # === å­å›¾ 5: ROC æ›²çº¿ ===
    from sklearn.metrics import roc_curve, auc
    ax5 = plt.subplot(2, 3, 5)

    all_losses = np.concatenate([member_losses, non_member_losses])
    all_labels = np.concatenate([np.ones(len(member_losses)), np.zeros(len(non_member_losses))])

    # æ³¨æ„ï¼šloss è¶Šå°è¶Šå¯èƒ½æ˜¯ memberï¼Œæ‰€ä»¥ç”¨ -loss
    fpr, tpr, _ = roc_curve(all_labels, -all_losses)
    roc_auc = auc(fpr, tpr)

    ax5.plot(fpr, tpr, color='#2E86AB', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
    ax5.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')
    ax5.set_xlabel('False Positive Rate', fontweight='bold')
    ax5.set_ylabel('True Positive Rate', fontweight='bold')
    ax5.set_title('ROC Curve', fontweight='bold')
    ax5.legend(loc="lower right")
    ax5.grid(True, alpha=0.3)

    # === å­å›¾ 6: ç»Ÿè®¡æ‘˜è¦è¡¨æ ¼ ===
    ax6 = plt.subplot(2, 3, 6)
    ax6.axis('off')

    from scipy import stats
    t_stat, p_value = stats.ttest_ind(member_losses, non_member_losses)
    pooled_std = np.sqrt((np.std(member_losses) ** 2 + np.std(non_member_losses) ** 2) / 2)
    cohens_d = (np.mean(member_losses) - np.mean(non_member_losses)) / pooled_std if pooled_std > 0 else 0

    loss_diff = np.mean(member_losses) - np.mean(non_member_losses)
    loss_diff_pct = (loss_diff / abs(np.mean(non_member_losses))) * 100 if np.mean(non_member_losses) != 0 else 0

    stats_data = [
        ['Metric', 'Member', 'Non-Member', 'Difference'],
        ['Mean Loss', f'{np.mean(member_losses):.4f}', f'{np.mean(non_member_losses):.4f}',
         f'{loss_diff:.4f}'],
        ['Std Loss', f'{np.std(member_losses):.4f}', f'{np.std(non_member_losses):.4f}', '-'],
        ['Median Loss', f'{np.median(member_losses):.4f}', f'{np.median(non_member_losses):.4f}',
         f'{np.median(member_losses) - np.median(non_member_losses):.4f}'],
        ['', '', '', ''],
        ['AUC', f'{roc_auc:.4f}', '', ''],
        ['P-value', f'{p_value:.6f}', '', ''],
        ['Cohen\'s d', f'{cohens_d:.4f}', '', ''],
        ['Rel.  Diff %', f'{loss_diff_pct:.2f}%', '', ''],
    ]

    table = ax6.table(cellText=stats_data, cellLoc='center', loc='center',
                      colWidths=[0.25, 0.25, 0.25, 0.25])
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 2)

    # è®¾ç½®è¡¨å¤´æ ·å¼
    for i in range(4):
        table[(0, i)].set_facecolor('#4ECDC4')
        table[(0, i)].set_text_props(weight='bold')

    ax6.set_title('Statistical Summary', fontweight='bold', fontsize=13, pad=20)

    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    output_path = os.path.join(output_dir, 'loss_analysis_v2.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ… åˆ†æå›¾å·²ä¿å­˜:  {output_path}")
    plt.close()


def save_results_json(results: Dict, output_dir: str):
    """ä¿å­˜ç»“æœä¸º JSON"""
    output_path = os.path.join(output_dir, 'loss_analysis_v2_results.json')

    json_results = {
        'member': {
            'num_samples': len(results['member']['texts']),
            'texts': results['member']['texts'],
            'mean_losses': [float(x) for x in results['member']['mean_losses']],
            'std_losses': [float(x) for x in results['member']['std_losses']],
            'overall_mean': float(np.mean(results['member']['mean_losses'])),
            'overall_std': float(np.std(results['member']['mean_losses'])),
        },
        'non_member': {
            'num_samples': len(results['non_member']['texts']),
            'texts': results['non_member']['texts'],
            'mean_losses': [float(x) for x in results['non_member']['mean_losses']],
            'std_losses': [float(x) for x in results['non_member']['std_losses']],
            'overall_mean': float(np.mean(results['non_member']['mean_losses'])),
            'overall_std': float(np.std(results['non_member']['mean_losses'])),
        }
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)

    print(f"âœ… ç»“æœ JSON å·²ä¿å­˜: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Loss Analysis V2 - Single Mask Level (5%)")
    parser.add_argument('-c', '--config', type=str,
                        default='attack/configs/config_fond.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num-samples', type=int, default=100,
                        help='æ¯ç±»é‡‡æ ·æ•°é‡')
    parser.add_argument('--mask-frac', type=float, default=0.90,
                        help='æ©ç æ¯”ä¾‹ (é»˜è®¤ 5%)')
    parser.add_argument('--num-runs', type=int, default=5,
                        help='æ¯ä¸ªæ ·æœ¬çš„é‡‡æ ·æ¬¡æ•°')
    parser.add_argument('--output', type=str, default='./case_study_v2_output',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--base-dir', type=str, default='./',
                        help='åŸºç¡€ç›®å½•')

    args = parser.parse_args()

    # åŠ è½½é…ç½®æ–‡ä»¶
    print("=" * 80)
    print("Loss Analysis V2 - Single Mask Level (5%)")
    print("=" * 80)
    print(f"\nğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")

    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    global_config = config.get('global', {})

    # è®¾ç½®éšæœºç§å­
    seed = global_config.get('seed', 42)
    set_seed(seed)
    print(f"âœ… éšæœºç§å­:  {seed}")

    # è§£ææ¨¡å‹è·¯å¾„
    load_from_base_dir = global_config.get('load_from_base_dir', False)
    model_path = global_config.get('target_model')
    tokenizer_path = global_config.get('tokenizer', model_path)

    if not model_path:
        raise ValueError("é…ç½®æ–‡ä»¶ä¸­å¿…é¡»æŒ‡å®š target_model")

    model_path = resolve_path(model_path, args.base_dir, load_from_base_dir)
    tokenizer_path = resolve_path(tokenizer_path, args.base_dir, load_from_base_dir)

    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")
    ModelManager.register_custom_models()

    device_str = global_config.get('device', 'cuda')
    device = torch.device(device_str)

    model, tokenizer, device = ModelManager.init_model(
        model_path, tokenizer_path, device
    )
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # åŠ è½½æ•°æ®é›†
    datasets_config = global_config.get('datasets', [])
    if not datasets_config:
        raise ValueError("é…ç½®æ–‡ä»¶ä¸­å¿…é¡»æŒ‡å®š datasets")

    ds_config = datasets_config[0]

    print(f"\nğŸ“¥ åŠ è½½æ•°æ®é›†...")

    if 'json_train_path' in ds_config and 'json_test_path' in ds_config:
        train_path = resolve_path(ds_config['json_train_path'], args.base_dir, load_from_base_dir)
        test_path = resolve_path(ds_config['json_test_path'], args.base_dir, load_from_base_dir)
        dataset = load_local_json_dataset(train_path, test_path)
        print(f"âœ… æœ¬åœ°æ•°æ®é›†åŠ è½½å®Œæˆ (æ€»æ ·æœ¬æ•°: {len(dataset)})")
    else:
        ds_name = ds_config.get('name') or ds_config.get('path')
        ds_split = ds_config.get('split', 'train')
        dataset = hf_load_dataset(ds_name, split=ds_split)
        print(f"âœ… HuggingFace æ•°æ®é›†åŠ è½½å®Œæˆ:  {ds_name} (æ€»æ ·æœ¬æ•°: {len(dataset)})")

    # åˆå§‹åŒ–åˆ†æå™¨
    analysis_config = {
        'mask_frac': args.mask_frac,
        'num_runs': args.num_runs,
        'max_length': global_config.get('max_length', 512),
    }

    analyzer = LossAnalyzer(model, tokenizer, device, analysis_config)

    # è¿è¡Œåˆ†æ
    print(f"\nğŸ”¬ å¼€å§‹åˆ†æ (æ¯ç±» {args.num_samples} ä¸ªæ ·æœ¬)...")
    results = analyzer.analyze_dataset(dataset, num_samples_per_class=args.num_samples)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("ğŸ“Š ç»Ÿè®¡æ‘˜è¦")
    print("=" * 80)

    member_losses = np.array(results['member']['mean_losses'])
    non_member_losses = np.array(results['non_member']['mean_losses'])

    print(f"\nâœ… MEMBER æ ·æœ¬:")
    print(f"  - æ ·æœ¬æ•°: {len(member_losses)}")
    print(f"  - å¹³å‡ Loss: {np.mean(member_losses):.6f}")
    print(f"  - æ ‡å‡†å·®: {np.std(member_losses):.6f}")
    print(f"  - ä¸­ä½æ•°: {np.median(member_losses):.6f}")
    print(f"  - èŒƒå›´: [{np.min(member_losses):.6f}, {np.max(member_losses):.6f}]")

    print(f"\nâŒ NON-MEMBER æ ·æœ¬:")
    print(f"  - æ ·æœ¬æ•°: {len(non_member_losses)}")
    print(f"  - å¹³å‡ Loss: {np.mean(non_member_losses):.6f}")
    print(f"  - æ ‡å‡†å·®: {np.std(non_member_losses):.6f}")
    print(f"  - ä¸­ä½æ•°: {np.median(non_member_losses):.6f}")
    print(f"  - èŒƒå›´:  [{np.min(non_member_losses):.6f}, {np.max(non_member_losses):.6f}]")

    # å¯¹æ¯”åˆ†æ
    loss_diff = np.mean(member_losses) - np.mean(non_member_losses)
    loss_diff_pct = (loss_diff / abs(np.mean(non_member_losses))) * 100 if np.mean(non_member_losses) != 0 else 0

    print(f"\nğŸ” å¯¹æ¯”åˆ†æ:")
    print(f"  - Loss å·®å¼‚ (M - NM): {loss_diff:.6f}")
    print(f"  - ç›¸å¯¹å·®å¼‚:  {loss_diff_pct:.2f}%")

    if loss_diff < 0:
        print(f"  âœ… Member Loss æ›´ä½ (ç¬¦åˆé¢„æœŸ)")
    else:
        print(f"  âš ï¸ Member Loss æ›´é«˜ (ä¸ç¬¦åˆé¢„æœŸ)")

    # ç»Ÿè®¡æ˜¾è‘—æ€§
    from scipy import stats
    from sklearn.metrics import roc_auc_score

    t_stat, p_value = stats.ttest_ind(member_losses, non_member_losses)
    pooled_std = np.sqrt((np.std(member_losses) ** 2 + np.std(non_member_losses) ** 2) / 2)
    cohens_d = (np.mean(member_losses) - np.mean(non_member_losses)) / pooled_std if pooled_std > 0 else 0

    all_losses = np.concatenate([member_losses, non_member_losses])
    all_labels = np.concatenate([np.ones(len(member_losses)), np.zeros(len(non_member_losses))])
    auc_score = roc_auc_score(all_labels, -all_losses)

    print(f"\nğŸ“ˆ ç»Ÿè®¡æ£€éªŒ:")
    print(f"  - T ç»Ÿè®¡é‡: {t_stat:.4f}")
    print(f"  - P å€¼: {p_value:.6f} {'(æ˜¾è‘—)' if p_value < 0.05 else '(ä¸æ˜¾è‘—)'}")
    print(f"  - Cohen's d: {cohens_d:.4f}")
    print(f"  - AUC: {auc_score:.4f}")

    if auc_score > 0.7:
        print(f"  âœ… åˆ†ç±»èƒ½åŠ›:  è¾ƒå¥½ (AUC > 0.7)")
    elif auc_score > 0.6:
        print(f"  âš ï¸ åˆ†ç±»èƒ½åŠ›: ä¸€èˆ¬ (0.6 < AUC â‰¤ 0.7)")
    else:
        print(f"  âŒ åˆ†ç±»èƒ½åŠ›:  è¾ƒå¼± (AUC â‰¤ 0.6)")

    # ç»˜å›¾å’Œä¿å­˜
    print(f"\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–...")
    plot_comprehensive_analysis(results, args.output, analysis_config)
    save_results_json(results, args.output)

    # å¿«é€Ÿæ‘˜è¦
    print(f"\n{'#' * 80}")
    print(f"# ğŸ“‹ å¿«é€Ÿæ‘˜è¦ (V2 - å•ä¸€æ©ç çº§åˆ«)")
    print(f"{'#' * 80}")
    print(f"""
é…ç½®: 
- æ©ç æ¯”ä¾‹: {args.mask_frac:.1%}
- é‡‡æ ·æ¬¡æ•°: {args.num_runs}
- æ¯ç±»æ ·æœ¬æ•°: {args.num_samples}

ç»“æœ:
- Member Loss:       {np.mean(member_losses):.6f} Â± {np.std(member_losses):.6f}
- Non-Member Loss:   {np.mean(non_member_losses):.6f} Â± {np.std(non_member_losses):.6f}
- å·®å¼‚:             {loss_diff:.6f} ({loss_diff_pct:.2f}%)
- AUC:             {auc_score:.4f}
- På€¼:             {p_value:.6f} {'(æ˜¾è‘—)' if p_value < 0.05 else '(ä¸æ˜¾è‘—)'}
- Cohen's d:       {cohens_d:.4f}

ç»“è®º:  {'âœ… æœ‰åŒºåˆ†æ€§' if auc_score > 0.6 and p_value < 0.05 else 'âŒ åŒºåˆ†æ€§ä¸è¶³'}
    """)
    print(f"{'#' * 80}\n")

    print("\n" + "=" * 80)
    print(f"âœ… åˆ†æå®Œæˆ!  æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°:  {args.output}")
    print("=" * 80)


if __name__ == '__main__':
    main()