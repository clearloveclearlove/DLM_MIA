run_name: LLaDA-8B-Base-pretrained-pretraining-lora-512-mimir-arxiv
train_mode: pretraining
wandb_project: Diff_LLM
wandb_group: SAMA

dataset:
  train:
    type: local
    path: /home1/yibiao/code/DLM-MIA/mimir-data/train/arxiv.json
  # val not provided -> We'll use test for val
  test:
    type: local
    path: /home1/yibiao/code/DLM-MIA/mimir-data/test/arxiv.json

tokenizer:
  identifier: /home1/yibiao/PTM/GSAI-ML-LLaDA-8B-Base 
  max_length: 512

model:
  identifier: /home1/yibiao/PTM/GSAI-ML-LLaDA-8B-Base 
  load_pretrained: true

lora:
  enabled: true # Set to true to enable LoRA
  r: 512
  lora_alpha: 1024
  lora_dropout: 0.05
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none" # "none", "all", or "lora_only"
  task_type: "CAUSAL_LM" # e.g., CAUSAL_LM, SEQ_2_SEQ_LM, TOKEN_CLS
  save_merged_model_at_end: true

training:
  output_dir: ./LLaDA-8B-Base-pretrained-pretraining-lora-512-mimir-arxiv-DSS-1_48_1.0e-5_15-real-20poch
  # deepspeed_config: configs/ds_config.json
  batch_size: 6
  per_device_train_batch_size: 2   
  gradient_accumulation_steps: 8
  learning_rate: 1.0e-5
  weight_decay: 0.1
  warmup_steps: 500
  num_train_epochs: 20         # For epoch-based training
  max_steps: -1                # Must be -1 to fully rely on epochs
  eval_strategy: epoch
  save_strategy: epoch
  logging_strategy: epoch
  save_total_limit: -1
  bf16: true
  fp16: false
  optim: "adamw_torch"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  #early_stopping_patience: 3
  #early_stopping_threshold: 0.001
