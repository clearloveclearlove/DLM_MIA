run_name: LLaDA-8B-Base-pretrained-pretraining-mimir-arxiv
train_mode: pretraining
wandb_project: Diff_LLM
wandb_group: SAMA

dataset:
  train:
    type: local
    path: /home1/yibiao/code/DLM-MIA/mimir-data/train/arxiv.json
  # val not provided -> We'll use test for val
  test:
    type: local
    path: /home1/yibiao/code/DLM-MIA/mimir-data/test/arxiv.json

tokenizer:
  identifier: /home1/yibiao/PTM/GSAI-ML-LLaDA-8B-Base 
  max_length: 512

model:
  identifier: /home1/yibiao/PTM/GSAI-ML-LLaDA-8B-Base 
  load_pretrained: true

training:
  output_dir: ./LLaDA-8B-Base-pretrained-pretraining-arxiv-1_48_5.0e-5_10poch
  deepspeed_config: /home1/yibiao/code/DLM-MIA/trainer/configs/ds_config.json
  batch_size: 8
  per_device_train_batch_size: 4   
  gradient_accumulation_steps: 6
  learning_rate: 1.0e-5
  weight_decay: 0.1
  warmup_steps: 500
  # gradient_checkpointing: true
  num_train_epochs: 10         # For epoch-based training
  max_steps: -1                # Must be -1 to fully rely on epochs
  eval_strategy: epoch
  save_strategy: epoch
  logging_strategy: epoch
  save_total_limit: -1
  bf16: true
  fp16: false
  optim: "adamw_torch"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  #early_stopping_patience: 3
  #early_stopping_threshold: 0.001
