#!/usr/bin/env python
"""
Case Study: Progressive Masking PPL Trends Analysis

åˆ†æž member å’Œ non-member æ ·æœ¬åœ¨æ¸è¿›å¼æŽ©ç ä¸‹çš„ PPL å˜åŒ–è¶‹åŠ¿
åŸºäºŽ config_fond.yaml çš„é…ç½®é£Žæ ¼
"""

import os
import sys
import argparse
import json
import yaml
from typing import List, Dict, Tuple
from pathlib import Path

import numpy as np
import torch
import torch.nn.functional as F
from datasets import Dataset, load_dataset as hf_load_dataset
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ç»˜å›¾é£Žæ ¼
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 11

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from attack.misc.models import ModelManager
from attack.misc.utils import set_seed, resolve_path
from attack.attacks.utils import get_model_nll_params


def load_local_json_dataset(train_path: str, test_path: str) -> Dataset:
    """
    åŠ è½½æœ¬åœ° JSON Lines æ•°æ®é›†

    æœŸæœ›æ ¼å¼ï¼ˆJSON Linesï¼‰ï¼š
    {"text": ".. .", ... }
    {"text": "...", ...}
    """
    from datasets import Dataset as HFDataset

    all_data = []

    def load_jsonl_file(file_path, default_label):
        """åŠ è½½ JSON Lines æ–‡ä»¶"""
        if not os.path.exists(file_path):
            print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨:  {file_path}")
            return []

        print(f"  ðŸ“‚ åŠ è½½:  {file_path}")
        items = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        item = json.loads(line)

                        # æå–æ–‡æœ¬
                        text = item.get('text')
                        if text is None:
                            print(f"     âš ï¸ ç¬¬ {line_num} è¡Œç¼ºå°‘ 'text' å­—æ®µ")
                            continue

                        # æå–æˆ–è®¾ç½®æ ‡ç­¾
                        label = item.get('label', default_label)

                        items.append({
                            'text': str(text),
                            'label': int(label)
                        })

                    except json.JSONDecodeError as e:
                        print(f"     âš ï¸ ç¬¬ {line_num} è¡Œ JSON è§£æžå¤±è´¥: {e}")
                        continue

                    # æ˜¾ç¤ºè¿›åº¦
                    if line_num % 1000 == 0:
                        print(f"     å·²åŠ è½½ {line_num} è¡Œ.. .", end='\r')

            print(f"     âœ… åŠ è½½ {len(items)} æ¡æ•°æ®" + " " * 20)

        except Exception as e:
            print(f"  âŒ åŠ è½½å¤±è´¥:  {e}")
            return []

        return items

    # åŠ è½½è®­ç»ƒé›†ï¼ˆmember, label=1ï¼‰
    print("\nåŠ è½½è®­ç»ƒé›† (Member):")
    train_items = load_jsonl_file(train_path, default_label=1)
    all_data.extend(train_items)

    # åŠ è½½æµ‹è¯•é›†ï¼ˆnon-member, label=0ï¼‰
    print("\nåŠ è½½æµ‹è¯•é›† (Non-member):")
    test_items = load_jsonl_file(test_path, default_label=0)
    all_data.extend(test_items)

    if not all_data:
        raise ValueError(f"""
æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®ã€‚è¯·æ£€æŸ¥ï¼š
1. æ–‡ä»¶æ˜¯å¦å­˜åœ¨: 
   - {train_path}
   - {test_path}
2. JSON Lines æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰
3. æ¯è¡Œæ˜¯å¦åŒ…å« 'text' å­—æ®µ
""")

    print(f"\nâœ… æ€»è®¡åŠ è½½ {len(all_data)} æ¡æ•°æ®")
    print(f"   - Member (label=1): {sum(1 for x in all_data if x['label'] == 1)}")
    print(f"   - Non-member (label=0): {sum(1 for x in all_data if x['label'] == 0)}")

    return HFDataset.from_list(all_data)


class PPLTrendAnalyzer:
    """PPL è¶‹åŠ¿åˆ†æžå™¨"""

    def __init__(self, model, tokenizer, device, config):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.config = config

        # èŽ·å–æ¨¡åž‹ç‰¹å®šå‚æ•°
        self.mask_id, self.shift_logits = get_model_nll_params(self.model)

        # æŽ©ç é…ç½®
        self.steps = config.get('steps', 10)
        self.min_mask_frac = config.get('min_mask_frac', 0.05)
        self.max_mask_frac = config.get('max_mask_frac', 0.90)
        self.mask_schedule = config.get('mask_schedule', 'linear')
        self.max_length = config.get('max_length', 512)

        print(f"[PPLTrendAnalyzer] åˆå§‹åŒ–å®Œæˆ")
        print(f"  - æ­¥æ•°: {self.steps}")
        print(f"  - æŽ©ç èŒƒå›´: [{self.min_mask_frac:.2f}, {self.max_mask_frac:.2f}]")  # â† ä¿®å¤è¿™é‡Œ
        print(f"  - è°ƒåº¦ç­–ç•¥: {self.mask_schedule}")
        print(f"  - Mask ID: {self.mask_id}, Shift Logits: {self.shift_logits}")
        print(f"  - Max Length: {self.max_length}")

    @torch.no_grad()
    def analyze_single_text(self, text: str) -> Tuple[List[float], List[float]]:
        """
        åˆ†æžå•ä¸ªæ–‡æœ¬çš„ PPL è¶‹åŠ¿

        Returns:
            (mask_fractions, ppl_values)
        """
        # Tokenize
        encoded = self.tokenizer.encode_plus(
            text,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=self.max_length
        ).to(self.device)

        input_ids = encoded["input_ids"]  # (1, L)
        attention_mask = encoded["attention_mask"].bool()  # (1, L)
        L = input_ids.size(1)

        # æœ‰æ•ˆé•¿åº¦
        valid_length = int(attention_mask.sum().item())
        if valid_length == 0:
            return [], []

        mask_fractions = []
        ppl_values = []
        cumulative_mask = torch.zeros_like(input_ids, dtype=torch.bool)

        # æ¸è¿›å¼æŽ©ç 
        for step in range(self.steps):
            # è®¡ç®—æŽ©ç æ¯”ä¾‹
            frac = self._compute_mask_fraction(step)
            mask_fractions.append(frac)

            # è®¡ç®—åº”æŽ©ç çš„æ€»æ•°
            desired_total = max(1, int(round(frac * valid_length)))
            current_total = int((cumulative_mask[0] & attention_mask[0]).sum().item())
            to_add = max(0, desired_total - current_total)

            # æ·»åŠ æ–°æŽ©ç 
            if to_add > 0:
                unmasked_valid = (~cumulative_mask[0]) & attention_mask[0]
                candidates = torch.where(unmasked_valid)[0]

                if candidates.numel() > 0:
                    to_add = min(to_add, candidates.numel())
                    perm = torch.randperm(candidates.numel(), device=self.device)
                    chosen = candidates[perm[:to_add]]
                    cumulative_mask[0, chosen] = True

            # åˆ›å»ºæŽ©ç è¾“å…¥
            masked_ids = input_ids.clone()
            masked_ids[cumulative_mask] = self.mask_id

            # å‰å‘ä¼ æ’­
            out = self.model(
                input_ids=masked_ids,
                attention_mask=attention_mask if not self.shift_logits else None
            )
            logits = out.logits if hasattr(out, 'logits') else out[0]

            if self.shift_logits:
                logits = torch.cat([logits[:, : 1, :], logits[:, :-1, :]], dim=1)

            # è®¡ç®—æŸå¤±ï¼ˆåªé’ˆå¯¹æŽ©ç ä½ç½®ï¼‰
            ce = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                input_ids.view(-1),
                reduction='none'
            ).view(1, L).float()

            # è®¡ç®— PPL
            masked_positions = cumulative_mask[0] & attention_mask[0]
            if masked_positions.sum() > 0:
                avg_loss = ce[0, masked_positions].mean().item()
                ppl = np.exp(avg_loss)
            else:
                ppl = float('inf')

            ppl_values.append(ppl)

        return mask_fractions, ppl_values

    def _compute_mask_fraction(self, step: int) -> float:
        """è®¡ç®—æŽ©ç æ¯”ä¾‹"""
        if self.mask_schedule == "geometric":
            if self.steps == 1:
                return self.max_mask_frac
            ratio = (self.max_mask_frac / max(self.min_mask_frac, 1e-6)) ** (1 / (self.steps - 1))
            frac = self.min_mask_frac * (ratio ** step)
        else:
            # çº¿æ€§
            if self.steps == 1:
                frac = self.max_mask_frac
            else:
                frac = self.min_mask_frac + (self.max_mask_frac - self.min_mask_frac) * step / (self.steps - 1)

        return min(max(frac, 0.0), 1.0)

    def analyze_dataset(self, dataset, num_samples_per_class: int = 10):
        """
        åˆ†æžæ•°æ®é›†ä¸­çš„æ ·æœ¬

        Args:
            dataset: åŒ…å« 'text' å’Œ 'label' çš„æ•°æ®é›†
            num_samples_per_class: æ¯ç±»é‡‡æ ·çš„æ ·æœ¬æ•°

        Returns:
            results: Dict with member/non-member results
        """
        results = {
            'member': {'texts': [], 'mask_fractions': [], 'ppl_sequences': [], 'slopes': []},
            'non_member': {'texts': [], 'mask_fractions': [], 'ppl_sequences': [], 'slopes': []}
        }

        # åˆ†ç¦» member å’Œ non-member
        member_indices = [i for i, label in enumerate(dataset['label']) if label == 1]
        non_member_indices = [i for i, label in enumerate(dataset['label']) if label == 0]

        print(f"\næ•°æ®é›†ç»Ÿè®¡:")
        print(f"  - Member æ ·æœ¬: {len(member_indices)}")
        print(f"  - Non-member æ ·æœ¬: {len(non_member_indices)}")

        # éšæœºé‡‡æ ·
        np.random.seed(42)
        selected_member = np.random.choice(member_indices,
                                           min(num_samples_per_class, len(member_indices)),
                                           replace=False)
        selected_non_member = np.random.choice(non_member_indices,
                                               min(num_samples_per_class, len(non_member_indices)),
                                               replace=False)

        # åˆ†æž member æ ·æœ¬
        print(f"\nåˆ†æž {len(selected_member)} ä¸ª Member æ ·æœ¬...")
        for idx in tqdm(selected_member, desc="Member"):
            text = dataset[int(idx)]['text']
            mask_fracs, ppl_vals = self.analyze_single_text(text)

            if len(ppl_vals) > 0:
                results['member']['texts'].append(text[: 100])
                results['member']['mask_fractions'].append(mask_fracs)
                results['member']['ppl_sequences'].append(ppl_vals)

                # è®¡ç®—æ–œçŽ‡
                slope = self._compute_slope(np.arange(len(ppl_vals)), np.array(ppl_vals))
                results['member']['slopes'].append(slope)

        # åˆ†æž non-member æ ·æœ¬
        print(f"åˆ†æž {len(selected_non_member)} ä¸ª Non-member æ ·æœ¬...")
        for idx in tqdm(selected_non_member, desc="Non-member"):
            text = dataset[int(idx)]['text']
            mask_fracs, ppl_vals = self.analyze_single_text(text)

            if len(ppl_vals) > 0:
                results['non_member']['texts'].append(text[:100])
                results['non_member']['mask_fractions'].append(mask_fracs)
                results['non_member']['ppl_sequences'].append(ppl_vals)

                # è®¡ç®—æ–œçŽ‡
                slope = self._compute_slope(np.arange(len(ppl_vals)), np.array(ppl_vals))
                results['non_member']['slopes'].append(slope)

        return results

    def _compute_slope(self, x, y):
        """çº¿æ€§å›žå½’è®¡ç®—æ–œçŽ‡"""
        if len(x) < 2:
            return 0.0

        x_mean = x.mean()
        y_mean = y.mean()
        numerator = ((x - x_mean) * (y - y_mean)).sum()
        denominator = ((x - x_mean) ** 2).sum()

        if denominator < 1e-10:
            return 0.0

        return numerator / denominator


def plot_comprehensive_analysis(results: Dict, output_dir: str):
    """ç»˜åˆ¶ç»¼åˆåˆ†æžå›¾"""
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»º 2x3 å­å›¾å¸ƒå±€
    fig = plt.figure(figsize=(18, 12))

    # === å­å›¾ 1: ä¸ªä½“ PPL æ›²çº¿ï¼ˆMemberï¼‰ ===
    ax1 = plt.subplot(2, 3, 1)
    for i, (mask_fracs, ppl_vals) in enumerate(zip(results['member']['mask_fractions'][:5],
                                                   results['member']['ppl_sequences'][:5])):
        ax1.plot(mask_fracs, ppl_vals, marker='o', alpha=0.6, label=f'Sample {i + 1}')
    ax1.set_xlabel('Mask Fraction')
    ax1.set_ylabel('Perplexity (PPL)')
    ax1.set_title('Member Samples - Individual PPL Curves (Top 5)')
    ax1.legend(fontsize=8)
    ax1.grid(True, alpha=0.3)

    # === å­å›¾ 2: ä¸ªä½“ PPL æ›²çº¿ï¼ˆNon-memberï¼‰ ===
    ax2 = plt.subplot(2, 3, 2)
    for i, (mask_fracs, ppl_vals) in enumerate(zip(results['non_member']['mask_fractions'][:5],
                                                   results['non_member']['ppl_sequences'][:5])):
        ax2.plot(mask_fracs, ppl_vals, marker='s', alpha=0.6, label=f'Sample {i + 1}')
    ax2.set_xlabel('Mask Fraction')
    ax2.set_ylabel('Perplexity (PPL)')
    ax2.set_title('Non-Member Samples - Individual PPL Curves (Top 5)')
    ax2.legend(fontsize=8)
    ax2.grid(True, alpha=0.3)

    # === å­å›¾ 3: å¹³å‡ PPL å¯¹æ¯”ï¼ˆå¸¦è¯¯å·®å¸¦ï¼‰ ===
    ax3 = plt.subplot(2, 3, 3)

    # Member å¹³å‡æ›²çº¿
    member_ppls = np.array(results['member']['ppl_sequences'])
    member_mean = np.mean(member_ppls, axis=0)
    member_std = np.std(member_ppls, axis=0)
    mask_fracs = results['member']['mask_fractions'][0]

    ax3.plot(mask_fracs, member_mean, marker='o', linewidth=2.5,
             label='Member (avg)', color='#2E86AB')
    ax3.fill_between(mask_fracs,
                     member_mean - member_std,
                     member_mean + member_std,
                     alpha=0.2, color='#2E86AB')

    # Non-member å¹³å‡æ›²çº¿
    non_member_ppls = np.array(results['non_member']['ppl_sequences'])
    non_member_mean = np.mean(non_member_ppls, axis=0)
    non_member_std = np.std(non_member_ppls, axis=0)

    ax3.plot(mask_fracs, non_member_mean, marker='s', linewidth=2.5,
             label='Non-Member (avg)', color='#A23B72')
    ax3.fill_between(mask_fracs,
                     non_member_mean - non_member_std,
                     non_member_mean + non_member_std,
                     alpha=0.2, color='#A23B72')

    ax3.set_xlabel('Mask Fraction', fontweight='bold')
    ax3.set_ylabel('Perplexity (PPL)', fontweight='bold')
    ax3.set_title('Average PPL Comparison (Mean Â± Std)', fontweight='bold', fontsize=13)
    ax3.legend(fontsize=11)
    ax3.grid(True, alpha=0.3)

    # === å­å›¾ 4: æ–œçŽ‡åˆ†å¸ƒå¯¹æ¯” (ç›´æ–¹å›¾) ===
    ax4 = plt.subplot(2, 3, 4)

    member_slopes = results['member']['slopes']
    non_member_slopes = results['non_member']['slopes']

    ax4.hist(member_slopes, bins=15, alpha=0.6, label='Member', color='#2E86AB', edgecolor='black')
    ax4.hist(non_member_slopes, bins=15, alpha=0.6, label='Non-Member', color='#A23B72', edgecolor='black')
    ax4.axvline(np.mean(member_slopes), color='#2E86AB', linestyle='--', linewidth=2,
                label=f'Member Mean: {np.mean(member_slopes):.2f}')
    ax4.axvline(np.mean(non_member_slopes), color='#A23B72', linestyle='--', linewidth=2,
                label=f'Non-Member Mean:  {np.mean(non_member_slopes):.2f}')
    ax4.set_xlabel('Slope (PPL Growth Rate)', fontweight='bold')
    ax4.set_ylabel('Frequency')
    ax4.set_title('Slope Distribution Comparison', fontweight='bold')
    ax4.legend(fontsize=9)
    ax4.grid(True, alpha=0.3, axis='y')

    # === å­å›¾ 5: æ–œçŽ‡åˆ†å¸ƒå¯¹æ¯” (ç®±çº¿å›¾) ===
    ax5 = plt.subplot(2, 3, 5)

    box_data = [member_slopes, non_member_slopes]
    bp = ax5.boxplot(box_data, labels=['Member', 'Non-Member'],
                     patch_artist=True, widths=0.6)

    # è®¾ç½®é¢œè‰²
    bp['boxes'][0].set_facecolor('#2E86AB')
    bp['boxes'][1].set_facecolor('#A23B72')

    for median in bp['medians']:
        median.set(color='red', linewidth=2)

    ax5.set_ylabel('Slope (PPL Growth Rate)', fontweight='bold')
    ax5.set_title('Slope Distribution (Box Plot)', fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')

    # æ·»åŠ å‡å€¼ç‚¹
    means = [np.mean(member_slopes), np.mean(non_member_slopes)]
    ax5.scatter([1, 2], means, color='yellow', s=100, zorder=3,
                edgecolors='black', linewidths=2, label='Mean')
    ax5.legend()

    # === å­å›¾ 6: ç»Ÿè®¡æ‘˜è¦è¡¨æ ¼ ===
    ax6 = plt.subplot(2, 3, 6)
    ax6.axis('off')

    # è®¡ç®—ç»Ÿè®¡é‡
    stats_data = [
        ['Metric', 'Member', 'Non-Member', 'Difference'],
        ['Mean Slope', f'{np.mean(member_slopes):.4f}', f'{np.mean(non_member_slopes):.4f}',
         f'{np.mean(member_slopes) - np.mean(non_member_slopes):.4f}'],
        ['Std Slope', f'{np.std(member_slopes):.4f}', f'{np.std(non_member_slopes):.4f}', '-'],
        ['Median Slope', f'{np.median(member_slopes):.4f}', f'{np.median(non_member_slopes):.4f}',
         f'{np.median(member_slopes) - np.median(non_member_slopes):.4f}'],
        ['', '', '', ''],
        ['Mean PPL@10%', f'{member_mean[0]:.2f}', f'{non_member_mean[0]:.2f}',
         f'{member_mean[0] - non_member_mean[0]:.2f}'],
        ['Mean PPL@50%', f'{member_mean[len(member_mean) // 2]:.2f}',
         f'{non_member_mean[len(non_member_mean) // 2]:.2f}',
         f'{member_mean[len(member_mean) // 2] - non_member_mean[len(non_member_mean) // 2]:.2f}'],
        ['Mean PPL@90%', f'{member_mean[-1]:.2f}', f'{non_member_mean[-1]:.2f}',
         f'{member_mean[-1] - non_member_mean[-1]:.2f}'],
    ]

    table = ax6.table(cellText=stats_data, cellLoc='center', loc='center',
                      colWidths=[0.25, 0.25, 0.25, 0.25])
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 2)

    # è®¾ç½®è¡¨å¤´æ ·å¼
    for i in range(4):
        table[(0, i)].set_facecolor('#4ECDC4')
        table[(0, i)].set_text_props(weight='bold')

    ax6.set_title('Statistical Summary', fontweight='bold', fontsize=13, pad=20)

    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    output_path = os.path.join(output_dir, 'ppl_slope_comprehensive_analysis.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ… ç»¼åˆåˆ†æžå›¾å·²ä¿å­˜:  {output_path}")

    plt.close()

    # === é¢å¤–ç»˜åˆ¶ï¼šPPL å¢žé•¿çŽ‡çƒ­åŠ›å›¾ ===
    plot_growth_rate_heatmap(results, output_dir)


def plot_growth_rate_heatmap(results: Dict, output_dir: str):
    """ç»˜åˆ¶ PPL å¢žé•¿çŽ‡çƒ­åŠ›å›¾"""
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # Member çƒ­åŠ›å›¾
    member_ppls = np.array(results['member']['ppl_sequences'])
    member_growth = np.diff(member_ppls, axis=1)  # è®¡ç®—å¢žé•¿çŽ‡

    im1 = axes[0].imshow(member_growth, aspect='auto', cmap='YlOrRd', interpolation='nearest')
    axes[0].set_xlabel('Mask Step Transition', fontweight='bold')
    axes[0].set_ylabel('Sample Index')
    axes[0].set_title('Member Samples - PPL Growth Rate Heatmap', fontweight='bold')
    plt.colorbar(im1, ax=axes[0], label='PPL Increase')

    # Non-member çƒ­åŠ›å›¾
    non_member_ppls = np.array(results['non_member']['ppl_sequences'])
    non_member_growth = np.diff(non_member_ppls, axis=1)

    im2 = axes[1].imshow(non_member_growth, aspect='auto', cmap='YlOrRd', interpolation='nearest')
    axes[1].set_xlabel('Mask Step Transition', fontweight='bold')
    axes[1].set_ylabel('Sample Index')
    axes[1].set_title('Non-Member Samples - PPL Growth Rate Heatmap', fontweight='bold')
    plt.colorbar(im2, ax=axes[1], label='PPL Increase')

    plt.tight_layout()
    output_path = os.path.join(output_dir, 'ppl_growth_rate_heatmap.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… PPL å¢žé•¿çŽ‡çƒ­åŠ›å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def save_results_json(results: Dict, output_dir: str):
    """ä¿å­˜ç»“æžœä¸º JSON"""
    output_path = os.path.join(output_dir, 'case_study_results.json')

    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    json_results = {
        'member': {
            'num_samples': len(results['member']['texts']),
            'texts': results['member']['texts'],
            'ppl_sequences': [list(map(float, seq)) for seq in results['member']['ppl_sequences']],
            'slopes': [float(s) for s in results['member']['slopes']],
            'mean_slope': float(np.mean(results['member']['slopes'])),
            'std_slope': float(np.std(results['member']['slopes'])),
        },
        'non_member': {
            'num_samples': len(results['non_member']['texts']),
            'texts': results['non_member']['texts'],
            'ppl_sequences': [list(map(float, seq)) for seq in results['non_member']['ppl_sequences']],
            'slopes': [float(s) for s in results['non_member']['slopes']],
            'mean_slope': float(np.mean(results['non_member']['slopes'])),
            'std_slope': float(np.std(results['non_member']['slopes'])),
        },
        'mask_fractions': results['member']['mask_fractions'][0] if results['member']['mask_fractions'] else []
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)

    print(f"âœ… ç»“æžœ JSON å·²ä¿å­˜: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="PPL Slope Case Study - åŸºäºŽ config_fond.yaml é£Žæ ¼")
    parser.add_argument('-c', '--config', type=str,
                        default='attack/configs/config_fond.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num-samples', type=int, default=20,
                        help='æ¯ç±»é‡‡æ ·æ•°é‡ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--steps', type=int, default=None,
                        help='æŽ©ç æ­¥æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--output', type=str, default='./case_study_output',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--base-dir', type=str, default='./',
                        help='åŸºç¡€ç›®å½•ï¼ˆç”¨äºŽè§£æžç›¸å¯¹è·¯å¾„ï¼‰')

    args = parser.parse_args()

    # === åŠ è½½é…ç½®æ–‡ä»¶ ===
    print("=" * 80)
    print("PPL Slope Case Study - Progressive Masking Analysis")
    print("=" * 80)
    print(f"\nðŸ“„ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")

    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    global_config = config.get('global', {})

    # è®¾ç½®éšæœºç§å­
    seed = global_config.get('seed', 42)
    set_seed(seed)
    print(f"âœ… éšæœºç§å­:  {seed}")

    # === è§£æžæ¨¡åž‹è·¯å¾„ ===
    load_from_base_dir = global_config.get('load_from_base_dir', False)
    model_path = global_config.get('target_model')
    tokenizer_path = global_config.get('tokenizer', model_path)

    if not model_path:
        raise ValueError("é…ç½®æ–‡ä»¶ä¸­å¿…é¡»æŒ‡å®š target_model")

    # è§£æžè·¯å¾„
    model_path = resolve_path(model_path, args.base_dir, load_from_base_dir)
    tokenizer_path = resolve_path(tokenizer_path, args.base_dir, load_from_base_dir)

    # === åŠ è½½æ¨¡åž‹ ===
    print(f"\nðŸ“¥ åŠ è½½æ¨¡åž‹: {model_path}")
    ModelManager.register_custom_models()

    device_str = global_config.get('device', 'cuda')
    device = torch.device(device_str)

    model, tokenizer, device = ModelManager.init_model(
        model_path, tokenizer_path, device
    )
    print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆ")

    # === åŠ è½½æ•°æ®é›† ===
    datasets_config = global_config.get('datasets', [])
    if not datasets_config:
        raise ValueError("é…ç½®æ–‡ä»¶ä¸­å¿…é¡»æŒ‡å®š datasets")

    ds_config = datasets_config[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®é›†

    print(f"\nðŸ“¥ åŠ è½½æ•°æ®é›†...")

    # æ£€æŸ¥æ˜¯æœ¬åœ° JSON è¿˜æ˜¯ HuggingFace æ•°æ®é›†
    if 'json_train_path' in ds_config and 'json_test_path' in ds_config:
        # æœ¬åœ° JSON æ ¼å¼
        train_path = resolve_path(ds_config['json_train_path'], args.base_dir, load_from_base_dir)
        test_path = resolve_path(ds_config['json_test_path'], args.base_dir, load_from_base_dir)

        dataset = load_local_json_dataset(train_path, test_path)
        print(f"âœ… æœ¬åœ°æ•°æ®é›†åŠ è½½å®Œæˆ (æ€»æ ·æœ¬æ•°: {len(dataset)})")
    else:
        # HuggingFace æ•°æ®é›†
        ds_name = ds_config.get('name') or ds_config.get('path')
        ds_split = ds_config.get('split', 'train')
        dataset = hf_load_dataset(ds_name, split=ds_split)
        print(f"âœ… HuggingFace æ•°æ®é›†åŠ è½½å®Œæˆ:  {ds_name} (æ€»æ ·æœ¬æ•°: {len(dataset)})")

    # === åˆå§‹åŒ–åˆ†æžå™¨ ===
    analysis_config = {
        'steps': args.steps if args.steps else global_config.get('steps', 10),
        'min_mask_frac': 0.05,
        'max_mask_frac': 0.90,
        'mask_schedule': 'linear',
        'max_length': global_config.get('max_length', 512),
    }

    analyzer = PPLTrendAnalyzer(model, tokenizer, device, analysis_config)

    # === è¿è¡Œåˆ†æž ===
    print(f"\nðŸ”¬ å¼€å§‹åˆ†æž (æ¯ç±» {args.num_samples} ä¸ªæ ·æœ¬)...")
    results = analyzer.analyze_dataset(dataset, num_samples_per_class=args.num_samples)

    # === æ‰“å°ç»Ÿè®¡ä¿¡æ¯ ===
    print("\n" + "=" * 80)
    print("ðŸ“Š ç»Ÿè®¡æ‘˜è¦")
    print("=" * 80)
    print(f"\nMember æ ·æœ¬:")
    print(f"  - åˆ†æžæ•°é‡: {len(results['member']['slopes'])}")
    print(f"  - å¹³å‡æ–œçŽ‡: {np.mean(results['member']['slopes']):.6f}")
    print(f"  - æ–œçŽ‡æ ‡å‡†å·®: {np.std(results['member']['slopes']):.6f}")
    print(f"  - æ–œçŽ‡ä¸­ä½æ•°: {np.median(results['member']['slopes']):.6f}")

    print(f"\nNon-Member æ ·æœ¬:")
    print(f"  - åˆ†æžæ•°é‡: {len(results['non_member']['slopes'])}")
    print(f"  - å¹³å‡æ–œçŽ‡: {np.mean(results['non_member']['slopes']):.6f}")
    print(f"  - æ–œçŽ‡æ ‡å‡†å·®: {np.std(results['non_member']['slopes']):.6f}")
    print(f"  - æ–œçŽ‡ä¸­ä½æ•°: {np.median(results['non_member']['slopes']):.6f}")

    slope_diff = np.mean(results['member']['slopes']) - np.mean(results['non_member']['slopes'])
    print(f"\nðŸ“ˆ æ–œçŽ‡å·®å¼‚ (Member - Non-Member): {slope_diff:.6f}")

    if slope_diff < 0:
        print("   âœ… Member æ ·æœ¬ PPL å¢žé•¿æ›´æ…¢ (ç¬¦åˆé¢„æœŸ)")
    else:
        print("   âš ï¸ Member æ ·æœ¬ PPL å¢žé•¿æ›´å¿« (ä¸ç¬¦åˆé¢„æœŸ)")

    # === ç»˜å›¾å’Œä¿å­˜ ===
    print(f"\nðŸŽ¨ ç”Ÿæˆå¯è§†åŒ–...")
    plot_comprehensive_analysis(results, args.output)
    save_results_json(results, args.output)

    print("\n" + "=" * 80)
    print(f"âœ… Case Study å®Œæˆ!  æ‰€æœ‰ç»“æžœå·²ä¿å­˜åˆ°:  {args.output}")
    print("=" * 80)


if __name__ == '__main__':
    main()